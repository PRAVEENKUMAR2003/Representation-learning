{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYmK3dxDUKQ1"
      },
      "outputs": [],
      "source": [
        "# Prerparing dataset triplet for siamese network\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (_, _) = mnist.load_data()\n",
        "\n",
        "num_classes = 10\n",
        "approx_samples_per_class = 15\n",
        "\n",
        "reshaped_x_train = []\n",
        "reshaped_y_train = []\n",
        "anchor_idx = []\n",
        "positive_idx = []\n",
        "negative_idx = []\n",
        "anchor_y = []\n",
        "positive_y = []\n",
        "negative_y = []\n",
        "\n",
        "for i in range(num_classes):\n",
        "    class_indices = np.where(y_train == i)[0]\n",
        "    num_samples = min(approx_samples_per_class, class_indices.shape[0])\n",
        "    selected_indices = np.random.choice(class_indices, size=num_samples, replace=False)\n",
        "    reshaped_x_train.append(x_train[selected_indices])\n",
        "    reshaped_y_train.append(y_train[selected_indices])\n",
        "\n",
        "reshaped_x_train = np.array(reshaped_x_train)\n",
        "reshaped_y_train = np.array(reshaped_y_train)\n",
        "x_train = reshaped_x_train\n",
        "print(\"Reshaped x_train shape:\", reshaped_x_train.shape)\n",
        "print(\"Reshaped y_train shape:\", reshaped_y_train.shape)\n",
        "for i in range(len(x_train)):\n",
        "    for j in range(len(x_train[i])):\n",
        "        for k in range(j+1, len(x_train[i])):\n",
        "            for l in range(len(x_train)):\n",
        "                if l==i:\n",
        "                    continue\n",
        "                for m in range(len(x_train[l])):\n",
        "                    anchor_idx.append(x_train[i,j])\n",
        "                    positive_idx.append(x_train[i,k])\n",
        "                    negative_idx.append(x_train[l,m])\n",
        "                    anchor_y.append(i)\n",
        "                    positive_y.append(i)\n",
        "                    negative_y.append(l)\n",
        "\n",
        "\n",
        "print(len(anchor_idx))\n",
        "anchor_idx=np.array(anchor_idx)\n",
        "positive_idx=np.array(positive_idx)\n",
        "negative_idx=np.array(negative_idx)\n",
        "anchor_y=np.array(anchor_y)\n",
        "positive_y=np.array(anchor_y)\n",
        "negative_y=np.array(negative_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import stuffs\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras\n",
        "from keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, Input, Flatten, Dense, Lambda, Reshape\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.models import Sequential\n"
      ],
      "metadata": {
        "id": "ZPO5cIbrnMp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbJsC9YyUPDl"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "def VGG16():\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Block 1\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(28,28,1)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Block 2\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Block 3\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Block 4\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Flatten\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dense(4096, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "feature = VGG16()\n",
        "image_a = Input(shape=(28,28), name='image_input_a')\n",
        "image_b = Input(shape=(28,28), name='image_input_b')\n",
        "image_c = Input(shape=(28,28), name='image_input_c')\n",
        "\n",
        "feature_vector_A = feature(image_a)\n",
        "feature_vector_B = feature(image_b)\n",
        "feature_vector_C = feature(image_c)\n",
        "\n",
        "def euclidean_distance(a, b):\n",
        "    return tf.sqrt(tf.reduce_sum(tf.square(a - b), axis=-1))\n",
        "\n",
        "class CustomLayer(keras.layers.Layer):\n",
        "    def custom_loss(self, output_intermediate_a, output_intermediate_b, output_intermediate_c):\n",
        "        output_a = output_intermediate_a\n",
        "        output_b = output_intermediate_b\n",
        "        output_c = output_intermediate_c\n",
        "        distance_positive = tf.reduce_sum(tf.square(output_a[:,0:1000] - output_b[:,0:1000]), axis=1)\n",
        "        distance_negative = tf.reduce_sum(tf.square(output_a[:,0:1000] - output_c[:,0:1000]), axis=1)\n",
        "        loss = tf.maximum(distance_positive - distance_negative + 0.1, 0.0)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "        return K.mean(loss)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        output_intermediate_a = inputs[0]\n",
        "        output_intermediate_b = inputs[1]\n",
        "        output_intermediate_c = inputs[2]\n",
        "        loss = self.custom_loss(output_intermediate_a, output_intermediate_b, output_intermediate_c)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        return inputs\n",
        "y = CustomLayer()([feature_vector_A, feature_vector_B, feature_vector_C])\n",
        "\n",
        "discriminator = Sequential([\n",
        "    Dense(10, input_shape=(4096,), activation='sigmoid')\n",
        "])\n",
        "\n",
        "class_vector_A = discriminator(y[0])\n",
        "class_vector_B = discriminator(y[1])\n",
        "class_vector_C = discriminator(y[2])\n",
        "\n",
        "\n",
        "model2 = keras.Model([image_a, image_b, image_c], [class_vector_A, class_vector_B, class_vector_C])\n",
        "\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "\n",
        "    y_true_onehot_1 = tf.one_hot(tf.cast(y_true[:,0], dtype=tf.int32), depth=10)\n",
        "    y_true_onehot_1 = tf.squeeze(y_true_onehot_1, axis=0)\n",
        "    y_pred_onehot_1 = y_pred[:,0]\n",
        "    loss1 = tf.keras.losses.CategoricalCrossentropy()(y_true_onehot_1, y_pred_onehot_1)\n",
        "    y_true_onehot_2 = tf.one_hot(tf.cast(y_true[:,1], dtype=tf.int32), depth=10)\n",
        "    y_true_onehot_2 = tf.squeeze(y_true_onehot_2, axis=0)\n",
        "    y_pred_onehot_2 = y_pred[:,1]\n",
        "    loss2 = tf.keras.losses.CategoricalCrossentropy()(y_true_onehot_2, y_pred_onehot_2)\n",
        "    y_true_onehot_3 = tf.one_hot(tf.cast(y_true[:,2], dtype=tf.int32), depth=10)\n",
        "    y_true_onehot_3 = tf.squeeze(y_true_onehot_3, axis=0)\n",
        "    y_pred_onehot_3 = y_pred[:,2]\n",
        "    loss3 = tf.keras.losses.CategoricalCrossentropy()(y_true_onehot_3, y_pred_onehot_3)\n",
        "\n",
        "    return loss1+loss2+loss3\n",
        "\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model2.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = keras.Model([image_a, image_b, image_c], [class_vector_A, class_vector_B, class_vector_C])\n"
      ],
      "metadata": {
        "id": "q8M7l6WxYoq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q_CNNgpppQs"
      },
      "outputs": [],
      "source": [
        "model2.compile(optimizer=tf.keras.optimizers.Adam(0.00005), loss = 'categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRbAVrvQT8iy"
      },
      "outputs": [],
      "source": [
        "model2.fit([anchor_idx,positive_idx,negative_idx], [to_categorical(anchor_y),to_categorical(positive_y),to_categorical(negative_y)], validation_split=0.2,epochs = 100, batch_size = 2048)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = Input(shape=(28,28), name='image_input')\n",
        "feature_vector = feature(image)\n",
        "class_vector = discriminator(feature_vector)\n",
        "model = keras.Model(image, class_vector)\n"
      ],
      "metadata": {
        "id": "Q2dLGppCdexm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "TadekhHmtOeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "5AnIzgYQ7eyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(anchor_idx, to_categorical(anchor_y), epochs = 100, batch_size = 2048,shuffle=True,validation_data=(x_test, to_categorical(y_test)))\n"
      ],
      "metadata": {
        "id": "TQXuNBCstZ7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAmxmKvlnhve"
      },
      "outputs": [],
      "source": [
        "filename = \"/content/drive/MyDrive/mnist/Siamesemodel_triple.joblib\"\n",
        "import joblib\n",
        "joblib.dump(model2, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n38OQFC3oI0A"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import custom_object_scope\n",
        "\n",
        "with custom_object_scope({'CustomLayer': CustomLayer}):\n",
        "    loaded_model = joblib.load('/content/drive/MyDrive/mnist/Siamesemodel_triple.joblib')\n",
        "model = loaded_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLlH35HMnkiA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQGZUuCgzr5j"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}